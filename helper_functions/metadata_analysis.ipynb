{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_csv_files = '../combined_results.csv'\n",
    "\n",
    "# Step 2: Load the CSV file\n",
    "df = pd.read_csv(input_csv_files)  # Replace 'your_file.csv' with your file path\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# input_csv_files = '../Analysis_Results/storage_server/All_models_res/final_results_all_models_V2.csv'\n",
    "\n",
    "# # Step 2: Load the CSV file\n",
    "# df = pd.read_csv(input_csv_files)  # Replace 'your_file.csv' with your file path\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# # Step 1: Check if \"Task_Language\" and \"code_task\" are in the columns\n",
    "# if \"Task_Language\" in df.columns and \"code_task\" in df.columns:\n",
    "#     # Step 2: Rearrange the columns\n",
    "#     cols = df.columns.tolist()  # Get the current order of columns\n",
    "#     cols.remove(\"Task_Language\")  # Remove \"Task_Language\" to reposition it\n",
    "#     cols.remove(\"code_task\")  # Remove \"code_task\" to reposition it\n",
    "#     new_order = [\"Task_Language\", \"code_task\"] + cols  # New order with \"Task_Language\" and \"code_task\" at the front\n",
    "    \n",
    "#     df = df[new_order]  # Rearrange the columns\n",
    "    \n",
    "#     # Step 3: Rename \"Task_Language\" to \"task_type\"\n",
    "#     df.rename(columns={\"Task_Language\": \"task_type\"}, inplace=True)\n",
    "#     df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "# else:\n",
    "#     print(\"Columns 'Task_Language' and/or 'code_task' not found in DataFrame\")\n",
    "\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    # Step 1: Drop columns that start with \"Unnamed:\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "\n",
    "    # Step 2: Re-order the columns\n",
    "    # Ensure \"task_type\", \"model_name\", and \"code_task\" are in the DataFrame\n",
    "    if all(col in df.columns for col in [\"task_type\", \"model_name\", \"code_task\"]):\n",
    "        # Remove these columns from the list to re-add them in the desired order\n",
    "        cols = df.columns.tolist()\n",
    "        for col in [\"task_type\", \"model_name\", \"code_task\"][::-1]:\n",
    "            cols.remove(col)\n",
    "            cols.insert(0, col)  # Insert at the beginning in reverse order to maintain the sequence\n",
    "        \n",
    "        df = df[cols]  # Re-order the DataFrame columns\n",
    "    else:\n",
    "        print(\"One or more of the required columns ('task_type', 'model_name', 'code_task') are missing.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df = preprocess_dataframe(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../storage_server/COLM_res_update/All_Models_res/final_results_all_models_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_first_n_lines(input_file, output_file, n):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = f.readlines()[:n]\n",
    "\n",
    "    json_objects = [json.loads(line) for line in lines]\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for obj in json_objects:\n",
    "            json.dump(obj, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "input_json_file_path = '../dataset/final_combined_data_standard.json'\n",
    "# output_json_file_path = '../dataset/sample.json'\n",
    "# Example usage\n",
    "# extract_first_n_lines(input_json_file_path, output_json_file_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_jsonl_file_path = '../dataset/final_combined_data_lines_true.jsonl'\n",
    "with open(input_json_file_path, 'r') as infile, open(output_jsonl_file_path, 'w') as outfile:\n",
    "    data = json.load(infile)\n",
    "    for entry in data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_jsonl(input_jsonl_file_path, output_sample_jsonl_file_path, n):\n",
    "    with open(input_jsonl_file_path, 'r') as infile, open(output_sample_jsonl_file_path, 'w') as outfile:\n",
    "        for _ in range(n):\n",
    "            line = infile.readline()\n",
    "            # Break if the file ends before reaching n lines\n",
    "            if not line:\n",
    "                break\n",
    "            outfile.write(line)\n",
    "\n",
    "# Example usage\n",
    "# input_jsonl_file_path = 'path_to_your_existing_jsonl_file.jsonl'\n",
    "output_sample_jsonl_file_path = 'path_to_your_sample_jsonl_file.jsonl'\n",
    "n = 10  # Number of lines you want in the sample\n",
    "create_sample_jsonl(output_jsonl_file_path, output_sample_jsonl_file_path, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5_infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
